{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipython notebook has code inspired from this page:\n",
    "https://github.com/kazuto1011/deeplab-pytorch/blob/master/demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Setting Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set paths\n",
    "val_image_dir = os.path.join(os.path.abspath('./'), 'dataset', 'coco', 'val2017')\n",
    "temporary_results = os.path.join(os.path.abspath('./'), 'temp_results')\n",
    "panoptic_ids_path = os.path.join(os.path.abspath('./'), \"git_repos\", \"panopticapi\", \"panoptic_coco_categories.json\")\n",
    "\n",
    "# Import usefull utilites\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from addict import Dict\n",
    "import cocostuffhelper as cocostuff\n",
    "import pandas as pd\n",
    "\n",
    "# Set paths\n",
    "os.chdir(os.path.abspath(\"./git_repos/deeplab-pytorch\"))\n",
    "cwd = os.path.abspath('./')\n",
    "trained_model_path = os.path.join(cwd, 'data', 'models', 'deeplabv2_resnet101_msc-cocostuff164k-100000.pth')\n",
    "label_ids_path = os.path.join(cwd, 'data', \"datasets\", \"cocostuff\", \"labels.txt\")\n",
    "\n",
    "# Import deep learning modules\n",
    "from libs.models import DeepLabV2_ResNet101_MSC\n",
    "from libs.utils import dense_crf\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trained_model_path, cuda=True):\n",
    "\n",
    "    # Load the model\n",
    "    model = DeepLabV2_ResNet101_MSC(n_classes=182)\n",
    "    state_dict = torch.load(trained_model_path, map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    # Create new OrderedDict that changes \"base\" to \"scale\"\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k.replace(\"base\",\"scale\")\n",
    "        name = name.replace(\"shortcut\",\"proj\")\n",
    "        name = name.replace(\"scale.aspp.c\",\"scale.aspp.stages.c\")\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # Make sure model is on \"eval\" mode and not on \"train\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make sure everything nicely is running on the GPU\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Running on\", torch.cuda.get_device_name(current_device))\n",
    "    else:\n",
    "        print(\"Running on CPU\") \n",
    "    model.to(device)\n",
    "    torch.set_grad_enabled(False) #Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n",
    "    \n",
    "    return model\n",
    "\n",
    "def semantic_prediction(model, image_name , crf=True, cuda=True):\n",
    "    \n",
    "    # Define devise on which image processing is going to be done\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    \n",
    "    # Image basic info\n",
    "    image_id = image_name.split('.')[0]\n",
    "    image_path = os.path.join(val_image_dir, image_name)\n",
    "    \n",
    "    # Image pre-processing\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR).astype(float)\n",
    "    image_original = np.array(image).astype(np.uint8) # crucial step for dense_crf\n",
    "    image = torch.from_numpy(image.transpose(2, 0, 1)).float().unsqueeze(0) # Convert to torch.Tensor and add \"batch\" axis\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Image instances prediction\n",
    "    pred = model(image)\n",
    "    pred = F.interpolate(\n",
    "        pred, size=image.shape[2:], mode=\"bilinear\", align_corners=True\n",
    "    )\n",
    "    pred = F.softmax(pred, dim=1)[0]\n",
    "    pred = pred.data.cpu().numpy()\n",
    "    \n",
    "    # Image post-processing\n",
    "    if crf:\n",
    "        #output = dense_crf(np.array(image_original).astype(np.uint8), output)\n",
    "        pred = dense_crf(image_original, pred)\n",
    "    semantic_map = np.argmax(pred, axis=0)\n",
    "    \n",
    "    # Convert output to coco result format\n",
    "    coco_semantics = cocostuff.segmentationToCocoResult(semantic_map, image_id)\n",
    "\n",
    "    return coco_semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_segmentation(trained_model_path, val_image_dir, cuda=True, crf=True):\n",
    "    \n",
    "    # Construct Deeplab model on GPU\n",
    "    model = build_model(trained_model_path, cuda)\n",
    "    \n",
    "    # Construct val2017 image name list\n",
    "    images = os.listdir(val_image_dir)\n",
    "\n",
    "    # Construct semantic predictions\n",
    "    predictions = []\n",
    "    for image_name in tqdm(images):\n",
    "        image_segmentation = semantic_prediction(model, image_name, crf)\n",
    "        predictions.extend(image_segmentation)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_semantic_results(predictions):\n",
    "    \n",
    "    # Encode predictions to be JSON serializable\n",
    "    print(\"decoding...\")\n",
    "    decoded_predictions = []\n",
    "    for prediction in tqdm(predictions):\n",
    "        prediction_dict = Dict(prediction)\n",
    "        prediction_dict.segmentation.counts = prediction_dict.segmentation.counts.decode('utf8')\n",
    "        decoded_predictions.append(prediction_dict)\n",
    "\n",
    "    # Matching between panoptic and stuff classes\n",
    "    print(\"matching...\")\n",
    "    converted_ids = pd.read_csv(r\"C:\\Users\\ENNAJIHYassin\\Desktop\\Panoptic\\converted_ids.csv\", index_col=[0])\n",
    "    matching_predictions = []\n",
    "    for prediction in tqdm(decoded_predictions):\n",
    "        prediction_dict = Dict(prediction)\n",
    "        stuff_id = prediction_dict.category_id\n",
    "        panoptic_id = converted_ids.loc[stuff_id+1,\"panoptic id\"]\n",
    "        #prediction_dict.category_id = int(panoptic_id) if not np.isnan(panoptic_id) else None\n",
    "        prediction_dict.category_id = int(panoptic_id) #if not np.isnan(panoptic_id) else None\n",
    "        matching_predictions.append(prediction_dict)\n",
    "    \n",
    "    # Save semantic segmentation into json file\n",
    "    print(\"saving...\")\n",
    "    with open('{}/{}.json'.format(temporary_results, \"Semantic_Segmentation\"), 'w') as outfile:\n",
    "        json.dump(matching_predictions, outfile)\n",
    "        \n",
    "    print(\"Succesfully saved predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GeForce GTX 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [31:12:02<00:00, 19.97s/it]     \n"
     ]
    }
   ],
   "source": [
    "predictions = semantic_segmentation(trained_model_path, val_image_dir, cuda=True, crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32829/32829 [00:00<00:00, 34963.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32829/32829 [00:00<00:00, 43645.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "Succesfully saved predictions!\n"
     ]
    }
   ],
   "source": [
    "save_semantic_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is legacy code! just to check results, not ment to be run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 139,\n",
       " 'category_id': 97,\n",
       " 'segmentation': {'size': [426, 640], 'counts': b'\\\\lk6110`[^1'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this code section was used to create the csv file that can do class matching between panoptic and semantic categories. \n",
    "\n",
    "### NOTE1: THIS CODE WAS RUN SEPARATLY IN SPYDER, NOT IN THIS NOTEBOOK!\n",
    "### NOTE2: THE CSV WAS STILL MODIFIED BY HAND AFTERWARDS!\n",
    "### NOTE3: MATCHING PANOPTIC AND SEMANTIC CLASSES IS A REAL STRUGGLE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stuff class labels into a dict\n",
    "with open(label_ids_path, 'r') as txt_file:\n",
    "    stuff_labels_dict = {}\n",
    "    for line in txt_file:\n",
    "        Id, name = line.strip().split('\\t')\n",
    "        stuff_labels_dict[str(int(Id) + 1)] = name\n",
    "\n",
    "# Read panoptic class labels into a dict\n",
    "with open(panoptic_ids_path) as json_file:\n",
    "    panoptic_categories = json.load(json_file)\n",
    "    panoptic_labels_dict = {}\n",
    "    for category in panoptic_categories:\n",
    "        name = category['name']\n",
    "        Id = category['id']\n",
    "        panoptic_labels_dict[str(Id)] = name\n",
    "\n",
    "\n",
    "stuff_ids = pd.DataFrame.from_dict(stuff_labels_dict, orient='index')\n",
    "panoptic_ids = pd.DataFrame.from_dict(panoptic_labels_dict, orient='index')\n",
    "\n",
    "for category in stuff_ids[0].values:\n",
    "    try:\n",
    "        stuff_ids.loc[stuff_ids[0]==category,\"panoptic id\"] = panoptic_ids.loc[[category==c for c in panoptic_ids[0]],:].index.values\n",
    "        stuff_ids.loc[stuff_ids[0]==category,\"panoptic category\"] = panoptic_ids.loc[[category==c for c in panoptic_ids[0]],0].values\n",
    "    except ValueError:\n",
    "        stuff_ids.loc[stuff_ids[0]==category,\"panoptic id\"] = np.nan\n",
    "        stuff_ids.loc[stuff_ids[0]==category,\"panoptic category\"] = np.nan\n",
    "                                                                                     \n",
    "no_stuff_ids = stuff_ids[stuff_ids[\"panoptic id\"].isnull()]\n",
    "                                                                                     \n",
    "for category in no_stuff_ids[0].values:\n",
    "    try:\n",
    "        no_stuff_ids.loc[no_stuff_ids[0]==category,\"panoptic id\"] = panoptic_ids.loc[[category[:3] in c for c in panoptic_ids[0]],:].index.values\n",
    "        no_stuff_ids.loc[no_stuff_ids[0]==category,\"panoptic category\"] = panoptic_ids.loc[[category[:3] in c for c in panoptic_ids[0]],0].values\n",
    "    except ValueError:\n",
    "        no_stuff_ids.loc[no_stuff_ids[0]==category,\"panoptic id\"] = np.nan\n",
    "        no_stuff_ids.loc[no_stuff_ids[0]==category,\"panoptic category\"] = np.nan\n",
    "\n",
    "no_stuff_ids_nan = no_stuff_ids[no_stuff_ids[\"panoptic id\"].isnull()]\n",
    "\n",
    "\"I make some manual adjustments and add some new classes\"\n",
    "\n",
    "no_id_categories = no_stuff_ids_nan[no_stuff_ids_nan[\"panoptic id\"].isnull()]\n",
    "\n",
    "converted_ids = pd.concat([stuff_ids.dropna(), no_stuff_ids.dropna(), no_stuff_ids_nan.dropna(), no_id_categories])\n",
    "\n",
    "converted_ids.to_csv(\"converted_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pred.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)\n",
    "    \n",
    "with open('pred.pkl', 'rb') as f:\n",
    "    loaded_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 139,\n",
       " 'category_id': 97,\n",
       " 'segmentation': {'size': [426, 640], 'counts': b'\\\\lk6110`[^1'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_predictions[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
